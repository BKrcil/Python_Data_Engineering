In this lab, you will use dockerized Hadoop to create a Hadoop Cluster, run a dockerized Cluster Hadoop instance and create a file in the HDFS (Hadoop Distributed File System), and view it on the GUI.


Clone the repository to your theia environment.

git clone https://github.com/ibm-developer-skills-network/ooxwv-docker_hadoop.git

Navigate to the docker-hadoop directory to build it.

cd ooxwv-docker_hadoop

Compose the docker application.

docker-compose up -d

Compose is a tool for defining and running multi-container Docker applications. It uses the YAML file to configure the serives and enables us to create and start all the services from just one configurtation file.

You will see that all the five containers are created and started.


Run the namenode as a mounted drive on bash.

docker exec -it namenode /bin/bash




Explore the hadoop environment
As you have learnt in the videos and reading thus far in the course, a Hadoop environment is configured by editing a set of configuration files:

hadoop-env.sh Serves as a master file to configure YARN, HDFS, MapReduce, and Hadoop-related project settings.

core-site.xml Defines HDFS and Hadoop core properties

hdfs-site.xml Governs the location for storing node metadata, fsimage file and log file.

mapred-site-xml Lists the parameters for MapReduce configuration.

yarn-site.xml Defines settings relevant to YARN. It contains configurations for the Node Manager, Resource Manager, Containers, and Application Master.

For the docker image, these xml files have been configured already. You can see these in the directory /opt/hadoop-3.2.1/etc/hadoop/ by running


ls /opt/hadoop-3.2.1/etc/hadoop/*.xml


Create a file in the HDFS
In the HDFS, create a directory structure named user/root/input.

hdfs dfs -mkdir -p /user/root/input

Copy all the hadoop configuration xml files into the input directory.

hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/root/input

Create a data.txt file in the current directory.

curl https://raw.githubusercontent.com/ibm-developer-skills-network/ooxwv-docker_hadoop/master/SampleMapReduce.txt --output data.txt

Copy the data.txt file into /user/root.

hdfs dfs -put data.txt /user/root/

Check if the file has been copied into the HDFS by viewing its content.

hdfs dfs -cat /user/root/data.txt
