Objectives

After completing this lab you will be able to:

Use the feature extractor CountVectorizer
Use the feature extractor TF-IDF
Use the feature transformer Tokenizer
Use the feature transformer StopWordsRemover
Use the feature transformer StringIndexer
Use the feature transformer StandardScaler
Datasets
In this lab you will be using dataset(s):

Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
from pyspark.sql import SparkSession
from pyspark.sql.functions import rand
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Feature Extraction and Transformation using Spark").getOrCreate()
24/07/22 01:49:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Task 1 - Tokenizer
A tokenizer is used to break a sentence into words.

#import tokenizer
from pyspark.ml.feature import Tokenizer
#create a sample dataframe
sentenceDataFrame = spark.createDataFrame([
    (1, "Spark is a distributed computing system."),
    (2, "It provides interfaces for multiple languages"),
    (3, "Spark is built on top of Hadoop")
], ["id", "sentence"])
#display the dataframe
sentenceDataFrame.show(truncate = False)
                                                                                
+---+---------------------------------------------+
|id |sentence                                     |
+---+---------------------------------------------+
|1  |Spark is a distributed computing system.     |
|2  |It provides interfaces for multiple languages|
|3  |Spark is built on top of Hadoop              |
+---+---------------------------------------------+

#create tokenizer instance.
#mention the column to be tokenized as inputcol
#mention the output column name where the tokens are to be stored.
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
#tokenize
token_df = tokenizer.transform(sentenceDataFrame)
#display the tokenized data
token_df.show(truncate=False)
+---+---------------------------------------------+----------------------------------------------------+
|id |sentence                                     |words                                               |
+---+---------------------------------------------+----------------------------------------------------+
|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |
|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|
|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |
+---+---------------------------------------------+----------------------------------------------------+

Task 2 - CountVectorizer
CountVectorizer is used to convert text into numerical format. It gives the count of each word in a given document.

#import CountVectorizer
from pyspark.ml.feature import CountVectorizer
#create a sample dataframe and display it.
textdata = [(1, "I love Spark Spark provides Python API ".split()),
            (2, "I love Python Spark supports Python".split()),
            (3, "Spark solves the big problem of big data".split())]
​
textdata = spark.createDataFrame(textdata, ["id", "words"])
​
textdata.show(truncate=False)
+---+-------------------------------------------------+
|id |words                                            |
+---+-------------------------------------------------+
|1  |[I, love, Spark, Spark, provides, Python, API]   |
|2  |[I, love, Python, Spark, supports, Python]       |
|3  |[Spark, solves, the, big, problem, of, big, data]|
+---+-------------------------------------------------+

# Create a CountVectorizer object
# mention the column to be count vectorized as inputcol
# mention the output column name where the count vectors are to be stored.
cv = CountVectorizer(inputCol="words", outputCol="features")
# Fit the CountVectorizer model on the input data
model = cv.fit(textdata)
                                                                                
# Transform the input data to bag-of-words vectors
result = model.transform(textdata)
# display the dataframe
result.show(truncate=False)
+---+-------------------------------------------------+----------------------------------------------------+
|id |words                                            |features                                            |
+---+-------------------------------------------------+----------------------------------------------------+
|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,4,8,12],[2.0,1.0,1.0,1.0,1.0,1.0])       |
|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,4,9],[1.0,2.0,1.0,1.0,1.0])              |
|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,3,5,6,7,10,11],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|
+---+-------------------------------------------------+----------------------------------------------------+

Task 3 - TF-IDF
Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word.

#import necessary classes for TF-IDF calculation
from pyspark.ml.feature import HashingTF, IDF, Tokenizer
​
#create a sample dataframe and display it.
sentenceData = spark.createDataFrame([
        (1, "Spark supports python"),
        (2, "Spark is fast"),
        (3, "Spark is easy")
    ], ["id", "sentence"])
​
sentenceData.show(truncate = False)
+---+---------------------+
|id |sentence             |
+---+---------------------+
|1  |Spark supports python|
|2  |Spark is fast        |
|3  |Spark is easy        |
+---+---------------------+

#tokenize the "sentence" column and store in the column "words"
tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
wordsData = tokenizer.transform(sentenceData)
wordsData.show(truncate = False)
+---+---------------------+-------------------------+
|id |sentence             |words                    |
+---+---------------------+-------------------------+
|1  |Spark supports python|[spark, supports, python]|
|2  |Spark is fast        |[spark, is, fast]        |
|3  |Spark is easy        |[spark, is, easy]        |
+---+---------------------+-------------------------+

# Create a HashingTF object
# mention the "words" column as input
# mention the "rawFeatures" column as output
​
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=10)
featurizedData = hashingTF.transform(wordsData)
​
featurizedData.show(truncate = False)
+---+---------------------+-------------------------+--------------------------+
|id |sentence             |words                    |rawFeatures               |
+---+---------------------+-------------------------+--------------------------+
|1  |Spark supports python|[spark, supports, python]|(10,[4,5,9],[1.0,1.0,1.0])|
|2  |Spark is fast        |[spark, is, fast]        |(10,[1,3,5],[1.0,1.0,1.0])|
|3  |Spark is easy        |[spark, is, easy]        |(10,[0,1,5],[1.0,1.0,1.0])|
+---+---------------------+-------------------------+--------------------------+

# Create an IDF object
# mention the "rawFeatures" column as input
# mention the "features" column as output
​
idf = IDF(inputCol="rawFeatures", outputCol="features")
idfModel = idf.fit(featurizedData)
tfidfData = idfModel.transform(featurizedData)
                                                                                
#display the tf-idf data
tfidfData.select("sentence", "features").show(truncate=False)
+---------------------+---------------------------------------------------------+
|sentence             |features                                                 |
+---------------------+---------------------------------------------------------+
|Spark supports python|(10,[4,5,9],[0.6931471805599453,0.0,0.6931471805599453]) |
|Spark is fast        |(10,[1,3,5],[0.28768207245178085,0.6931471805599453,0.0])|
|Spark is easy        |(10,[0,1,5],[0.6931471805599453,0.28768207245178085,0.0])|
+---------------------+---------------------------------------------------------+

Task 4 - StopWordsRemover
StopWordsRemover is a transformer that filters out stop words like "a","an" and "the".

#import StopWordsRemover
from pyspark.ml.feature import StopWordsRemover
#create a dataframe with sample text and display it
textData = spark.createDataFrame([
    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),
    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),
    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])
], ["id", "sentence"])
​
textData.show(truncate = False)
+---+------------------------------------------------------------+
|id |sentence                                                    |
+---+------------------------------------------------------------+
|1  |[Spark, is, an, open-source, distributed, computing, system]|
|2  |[IT, has, interfaces, for, multiple, languages]             |
|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |
+---+------------------------------------------------------------+

# remove stopwords from "sentence" column and store the result in "filtered_sentence" column
remover = StopWordsRemover(inputCol="sentence", outputCol="filtered_sentence")
textData = remover.transform(textData)
# display the dataframe
textData.show(truncate = False)
+---+------------------------------------------------------------+----------------------------------------------------+
|id |sentence                                                    |filtered_sentence                                   |
+---+------------------------------------------------------------+----------------------------------------------------+
|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|
|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |
|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |
+---+------------------------------------------------------------+----------------------------------------------------+

Task 5 - StringIndexer
StringIndexer converts a column of strings into a column of integers.

#import StringIndexer
from pyspark.ml.feature import StringIndexer
#create a dataframe with sample text and display it
colors = spark.createDataFrame(
    [(0, "red"), (1, "red"), (2, "blue"), (3, "yellow" ), (4, "yellow"), (5, "yellow")],
    ["id", "color"])
​
colors.show()
+---+------+
| id| color|
+---+------+
|  0|   red|
|  1|   red|
|  2|  blue|
|  3|yellow|
|  4|yellow|
|  5|yellow|
+---+------+

# index the strings in the column "color" and store their indexes in the column "colorIndex"
indexer = StringIndexer(inputCol="color", outputCol="colorIndex")
indexed = indexer.fit(colors).transform(colors)
# display the dataframe
indexed.show()
+---+------+----------+
| id| color|colorIndex|
+---+------+----------+
|  0|   red|       1.0|
|  1|   red|       1.0|
|  2|  blue|       2.0|
|  3|yellow|       0.0|
|  4|yellow|       0.0|
|  5|yellow|       0.0|
+---+------+----------+

Task 6 - StandardScaler
StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1

#import StandardScaler
from pyspark.ml.feature import StandardScaler
​
# Create a sample dataframe and display it
from pyspark.ml.linalg import Vectors
data = [(1, Vectors.dense([70, 170, 17])),
        (2, Vectors.dense([80, 165, 25])),
        (3, Vectors.dense([65, 150, 135]))]
df = spark.createDataFrame(data, ["id", "features"])
​
df.show()
+---+------------------+
| id|          features|
+---+------------------+
|  1| [70.0,170.0,17.0]|
|  2| [80.0,165.0,25.0]|
|  3|[65.0,150.0,135.0]|
+---+------------------+

# Define the StandardScaler transformer
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withStd=True, withMean=True)
# Fit the transformer to the dataset
scalerModel = scaler.fit(df)
# Scale the data
scaledData = scalerModel.transform(df)
# Show the scaled data
scaledData.show(truncate = False)
+---+------------------+------------------------------------------------------------+
|id |features          |scaledFeatures                                              |
+---+------------------+------------------------------------------------------------+
|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254366,-0.6369487984517485] |
|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.32025630761017515,-0.5156252177942725]|
|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021]  |
+---+------------------+------------------------------------------------------------+

Stop Spark Session

spark.stop()
Exercises
Create Spark Session

#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Exercises - Feature Extraction and Transformation using Spark").getOrCreate()
Create Dataframes

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/proverbs.csv
​
--2024-07-22 01:53:52--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/proverbs.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 846 [text/csv]
Saving to: ‘proverbs.csv’

proverbs.csv        100%[===================>]     846  --.-KB/s    in 0s      

2024-07-22 01:53:52 (6.84 MB/s) - ‘proverbs.csv’ saved [846/846]

# Load proverbs dataset
textdata = spark.read.csv("proverbs.csv", header=True, inferSchema=True)
# display dataframe
textdata.show(truncate = False)
+---+-----------------------------------------------------------+
|id |text                                                       |
+---+-----------------------------------------------------------+
|1  |When in Rome do as the Romans do.                          |
|2  |Do not judge a book by its cover.                          |
|3  |Actions speak louder than words.                           |
|4  |A picture is worth a thousand words.                       |
|5  |If at first you do not succeed try try again.              |
|6  |Practice makes perfect.                                    |
|7  |An apple a day keeps the doctor away.                      |
|8  |When the going gets tough the tough get going.             |
|9  |All is fair in love and war.                               |
|10 |Too many cooks spoil the broth.                            |
|11 |You can not make an omelette without breaking eggs.        |
|12 |The early bird catches the worm.                           |
|13 |Better late than never.                                    |
|14 |Honesty is the best policy.                                |
|15 |A penny saved is a penny earned.                           |
|16 |Two wrongs do not make a right.                            |
|17 |The grass is always greener on the other side of the fence.|
|18 |Do not count your chickens before they're hatched.         |
|19 |Laughter is the best medicine.                             |
|20 |Rome wasn't built in a day.                                |
+---+-----------------------------------------------------------+
only showing top 20 rows

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg.csv
​
--2024-07-22 01:54:03--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13891 (14K) [text/csv]
Saving to: ‘mpg.csv.4’

mpg.csv.4           100%[===================>]  13.57K  --.-KB/s    in 0s      

2024-07-22 01:54:03 (50.5 MB/s) - ‘mpg.csv.4’ saved [13891/13891]

# Load mpg dataset
mpgdata = spark.read.csv("mpg.csv", header=True, inferSchema=True)
# display dataframe
mpgdata.show()
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|
|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|
|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|
|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|
|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|
|15.0|        8|      350.0|       165|  3693|      11.5|  70|American|
|18.0|        8|      307.0|       130|  3504|      12.0|  70|American|
|14.0|        8|      454.0|       220|  4354|       9.0|  70|American|
|15.0|        8|      400.0|       150|  3761|       9.5|  70|American|
|10.0|        8|      307.0|       200|  4376|      15.0|  70|American|
|15.0|        8|      383.0|       170|  3563|      10.0|  70|American|
|11.0|        8|      318.0|       210|  4382|      13.5|  70|American|
|10.0|        8|      360.0|       215|  4615|      14.0|  70|American|
|15.0|        8|      429.0|       198|  4341|      10.0|  70|American|
|21.0|        6|      200.0|        85|  2587|      16.0|  70|American|
|17.0|        8|      302.0|       140|  3449|      10.5|  70|American|
| 9.0|        8|      304.0|       193|  4732|      18.5|  70|American|
|14.0|        8|      340.0|       160|  3609|       8.0|  70|American|
|22.0|        6|      198.0|        95|  2833|      15.5|  70|American|
|14.0|        8|      440.0|       215|  4312|       8.5|  70|American|
+----+---------+-----------+----------+------+----------+----+--------+
only showing top 20 rows

Exercise 1 - Tokenizer
#display the dataframe
textdata.show(truncate = False)
+---+-----------------------------------------------------------+
|id |text                                                       |
+---+-----------------------------------------------------------+
|1  |When in Rome do as the Romans do.                          |
|2  |Do not judge a book by its cover.                          |
|3  |Actions speak louder than words.                           |
|4  |A picture is worth a thousand words.                       |
|5  |If at first you do not succeed try try again.              |
|6  |Practice makes perfect.                                    |
|7  |An apple a day keeps the doctor away.                      |
|8  |When the going gets tough the tough get going.             |
|9  |All is fair in love and war.                               |
|10 |Too many cooks spoil the broth.                            |
|11 |You can not make an omelette without breaking eggs.        |
|12 |The early bird catches the worm.                           |
|13 |Better late than never.                                    |
|14 |Honesty is the best policy.                                |
|15 |A penny saved is a penny earned.                           |
|16 |Two wrongs do not make a right.                            |
|17 |The grass is always greener on the other side of the fence.|
|18 |Do not count your chickens before they're hatched.         |
|19 |Laughter is the best medicine.                             |
|20 |Rome wasn't built in a day.                                |
+---+-----------------------------------------------------------+
only showing top 20 rows

Write code to tokenize the "text" column of the "textdata" dataframe and store the tokens in the column "words"

# your code goes here
from pyspark.ml.feature import Tokenizer
​
tokenizer = Tokenizer(inputCol="text", outputCol="words")
​
textdata = tokenizer.transform(textdata)

#display the tokenized data
textdata.select("id","words").show(truncate=False)
+---+------------------------------------------------------------------------+
|id |words                                                                   |
+---+------------------------------------------------------------------------+
|1  |[when, in, rome, do, as, the, romans, do.]                              |
|2  |[do, not, judge, a, book, by, its, cover.]                              |
|3  |[actions, speak, louder, than, words.]                                  |
|4  |[a, picture, is, worth, a, thousand, words.]                            |
|5  |[if, at, first, you, do, not, succeed, try, try, again.]                |
|6  |[practice, makes, perfect.]                                             |
|7  |[an, apple, a, day, keeps, the, doctor, away.]                          |
|8  |[when, the, going, gets, tough, the, tough, get, going.]                |
|9  |[all, is, fair, in, love, and, war.]                                    |
|10 |[too, many, cooks, spoil, the, broth.]                                  |
|11 |[you, can, not, make, an, omelette, without, breaking, eggs.]           |
|12 |[the, early, bird, catches, the, worm.]                                 |
|13 |[better, late, than, never.]                                            |
|14 |[honesty, is, the, best, policy.]                                       |
|15 |[a, penny, saved, is, a, penny, earned.]                                |
|16 |[two, wrongs, do, not, make, a, right.]                                 |
|17 |[the, grass, is, always, greener, on, the, other, side, of, the, fence.]|
|18 |[do, not, count, your, chickens, before, they're, hatched.]             |
|19 |[laughter, is, the, best, medicine.]                                    |
|20 |[rome, wasn't, built, in, a, day.]                                      |
+---+------------------------------------------------------------------------+
only showing top 20 rows

Exercise 2 - CountVectorizer
CountVectorize the column "words" of the "textdata" dataframe and store the result in the column "features"

# your code goes here
from pyspark.ml.feature import CountVectorizer
​
cv = CountVectorizer(inputCol="words", outputCol="features")
​
model = cv.fit(textdata)
​
textdata = model.transform(textdata)

# Show the resulting dataframe
textdata.select("words","features").show(truncate=False)
Exercise 3 - StringIndexer
Convert the string column "Origin" to a numeric column "OriginIndex" in the dataframe "mpgdata"

# your code goes here
from pyspark.ml.feature import StringIndexer
indexer = StringIndexer(inputCol="Origin", outputCol="OriginIndex")
indexed = indexer.fit(mpgdata).transform(mpgdata)

#show the dataframe
​
indexed.orderBy(rand()).show()
​
+----+---------+-----------+----------+------+----------+----+--------+-----------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|OriginIndex|
+----+---------+-----------+----------+------+----------+----+--------+-----------+
|38.0|        4|      105.0|        63|  2125|      14.7|  82|American|        0.0|
|16.0|        8|      318.0|       150|  4190|      13.0|  76|American|        0.0|
|18.0|        8|      318.0|       150|  3436|      11.0|  70|American|        0.0|
|24.0|        4|      134.0|        96|  2702|      13.5|  75|Japanese|        1.0|
|20.6|        6|      231.0|       105|  3380|      15.8|  78|American|        0.0|
|18.1|        8|      302.0|       139|  3205|      11.2|  78|American|        0.0|
|18.0|        6|      250.0|       105|  3459|      16.0|  75|American|        0.0|
|18.0|        6|      225.0|       105|  3121|      16.5|  73|American|        0.0|
|39.4|        4|       85.0|        70|  2070|      18.6|  78|Japanese|        1.0|
|16.0|        6|      250.0|       105|  3897|      18.5|  75|American|        0.0|
|15.0|        8|      400.0|       150|  3761|       9.5|  70|American|        0.0|
|18.0|        6|      250.0|        78|  3574|      21.0|  76|American|        0.0|
|20.3|        5|      131.0|       103|  2830|      15.9|  78|European|        2.0|
|29.9|        4|       98.0|        65|  2380|      20.7|  81|American|        0.0|
|39.0|        4|       86.0|        64|  1875|      16.4|  81|American|        0.0|
|11.0|        8|      318.0|       210|  4382|      13.5|  70|American|        0.0|
|13.0|        8|      318.0|       150|  3755|      14.0|  76|American|        0.0|
|29.0|        4|       97.0|        75|  2171|      16.0|  75|Japanese|        1.0|
|24.0|        4|      120.0|        97|  2489|      15.0|  74|Japanese|        1.0|
|34.1|        4|       86.0|        65|  1975|      15.2|  79|Japanese|        1.0|
+----+---------+-----------+----------+------+----------+----+--------+-----------+
only showing top 20 rows

Exercise 4 - StandardScaler
Create a single column named "feaures" using the columns "Cylinders", "Engine Disp", "Horsepower", "Weight"

from pyspark.ml.feature import VectorAssembler
assembler = VectorAssembler(inputCols=["Cylinders", "Engine Disp", "Horsepower", "Weight"], outputCol="features")
​
mpg_transformed_data = assembler.transform(mpgdata)
​
#show the dataframe
mpg_transformed_data.select("MPG","features").show(truncate = False)
+----+------------------------+
|MPG |features                |
+----+------------------------+
|15.0|[8.0,390.0,190.0,3850.0]|
|21.0|[6.0,199.0,90.0,2648.0] |
|18.0|[6.0,199.0,97.0,2774.0] |
|16.0|[8.0,304.0,150.0,3433.0]|
|14.0|[8.0,455.0,225.0,3086.0]|
|15.0|[8.0,350.0,165.0,3693.0]|
|18.0|[8.0,307.0,130.0,3504.0]|
|14.0|[8.0,454.0,220.0,4354.0]|
|15.0|[8.0,400.0,150.0,3761.0]|
|10.0|[8.0,307.0,200.0,4376.0]|
|15.0|[8.0,383.0,170.0,3563.0]|
|11.0|[8.0,318.0,210.0,4382.0]|
|10.0|[8.0,360.0,215.0,4615.0]|
|15.0|[8.0,429.0,198.0,4341.0]|
|21.0|[6.0,200.0,85.0,2587.0] |
|17.0|[8.0,302.0,140.0,3449.0]|
|9.0 |[8.0,304.0,193.0,4732.0]|
|14.0|[8.0,340.0,160.0,3609.0]|
|22.0|[6.0,198.0,95.0,2833.0] |
|14.0|[8.0,440.0,215.0,4312.0]|
+----+------------------------+
only showing top 20 rows

Use StandardScaler to scale the "features" column of the dataframe "mpg_transformed_data" and save the scaled data into the "scaledFeatures" column.

# your code goes here
from pyspark.ml.feature import StandardScaler
​
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withStd=True, withMean=True)
​
scalerModel = scaler.fit(mpg_transformed_data)
​
scaledData = scalerModel.transform(mpg_transformed_data)

# Show the scaled data
scaledData.select("features","scaledFeatures").show(truncate = False)
+------------------------+-----------------------------------------------------------------------------------+
|features                |scaledFeatures                                                                     |
+------------------------+-----------------------------------------------------------------------------------+
|[8.0,390.0,190.0,3850.0]|[1.48205302652896,1.869079955831451,2.222084561602166,1.027093462353608]           |
|[6.0,199.0,90.0,2648.0] |[0.3095711165403583,0.043843985634147174,-0.37591456792553746,-0.38801882543985255]|
|[6.0,199.0,97.0,2774.0] |[0.3095711165403583,0.043843985634147174,-0.1940546288585982,-0.2396792678175763]  |
|[8.0,304.0,150.0,3433.0]|[1.48205302652896,1.0472459587792617,1.1828849097910845,0.5361601645084557]        |
|[8.0,455.0,225.0,3086.0]|[1.48205302652896,2.4902335582546176,3.131384256936862,0.12763773200901246]        |
|[8.0,350.0,165.0,3693.0]|[1.48205302652896,1.4868315851095026,1.57258477922024,0.8422576643639463]          |
|[8.0,307.0,130.0,3504.0]|[1.48205302652896,1.0759145865834079,0.6632850838855439,0.619748327930532]         |
|[8.0,454.0,220.0,4354.0]|[1.48205302652896,2.480677348986569,3.001484300460477,1.6204516928427128]          |
|[8.0,400.0,150.0,3761.0]|[1.48205302652896,1.964642048511938,1.1828849097910845,0.9223139335569208]         |
|[8.0,307.0,200.0,4376.0]|[1.48205302652896,1.0759145865834079,2.481884474554936,1.646352250522793]          |
|[8.0,383.0,170.0,3563.0]|[1.48205302652896,1.80218649095511,1.7024847356966253,0.689208914436201]           |
|[8.0,318.0,210.0,4382.0]|[1.48205302652896,1.1810328885319439,2.7416843875077066,1.6534160389809966]        |
|[8.0,360.0,215.0,4615.0]|[1.48205302652896,1.5823936777899896,2.871584343984092,1.9277264907745708]         |
|[8.0,429.0,198.0,4341.0]|[1.48205302652896,2.241772117285351,2.4299244919643823,1.6051468178499384]         |
|[6.0,200.0,85.0,2587.0] |[0.3095711165403583,0.053400194902195885,-0.5058145244019226,-0.4598340080982561]  |
|[8.0,302.0,140.0,3449.0]|[1.48205302652896,1.0281335402431644,0.9230849968383142,0.5549969337303321]        |
|[8.0,304.0,193.0,4732.0]|[1.48205302652896,1.0472459587792617,2.300024535487997,2.0654703657095417]         |
|[8.0,340.0,160.0,3609.0]|[1.48205302652896,1.3912694924290154,1.442684822743855,0.7433646259490956]         |
|[6.0,198.0,95.0,2833.0] |[0.3095711165403583,0.03428777636609846,-0.24601461144915227,-0.17021868131190726] |
|[8.0,440.0,215.0,4312.0]|[1.48205302652896,2.3468904192338864,2.871584343984092,1.5710051736352875]         |
+------------------------+-----------------------------------------------------------------------------------+
only showing top 20 rows

Stop Spark Session

spark.stop()






