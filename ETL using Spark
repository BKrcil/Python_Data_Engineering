Objectives
After completing this lab you will be able to:

Create a Spark Dataframe from the raw data and write to CSV file.
Read from a csv file and write to parquet file
Condense PARQUET to a single file.
Read from a parquet file and write to csv file
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
from pyspark.sql import SparkSession
​
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("ETL using Spark").getOrCreate()
24/07/21 00:21:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Task 1 - Create a Dataframe from the raw data and write to CSV file.
#create a list of tuples
#each tuple contains the student id, height and weight
data = [("student1",64,90),
        ("student2",59,100),
        ("student3",69,95),
        ("",70,110),
        ("student5",60,80),
        ("student3",69,95),
        ("student6",62,85),
        ("student7",65,80),
        ("student7",65,80)]
​
# some rows are intentionally duplicated
#create a dataframe using createDataFrame and pass the data and the column names.
​
df = spark.createDataFrame(data, ["student","height_inches","weight_pounds"])
# show the data frame
​
df.show()
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student1|           64|           90|
|student2|           59|          100|
|student3|           69|           95|
|        |           70|          110|
|student5|           60|           80|
|student3|           69|           95|
|student6|           62|           85|
|student7|           65|           80|
|student7|           65|           80|
+--------+-------------+-------------+

Write to csv file

Note: In Apache Spark, when you use the write method to save a DataFrame to a CSV file, it indeed creates a directory rather than a single file. This is because Spark is designed to run in a distributed manner across multiple nodes, and it saves the output as multiple part files within a directory.The csv file is within the directory.

df.write.mode("overwrite").csv("student-hw.csv", header=True)
                                                                                
#If you do not wish to over write use df.write.csv("student-hw.csv", header=True)
Verify the csv file

# Load student dataset
df = spark.read.csv("student-hw.csv", header=True, inferSchema=True)
​
# display dataframe
df.show()
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student7|           65|           80|
|student7|           65|           80|
|student2|           59|          100|
|student1|           64|           90|
|student3|           69|           95|
|student5|           60|           80|
|student3|           69|           95|
|student6|           62|           85|
|    null|           70|          110|
+--------+-------------+-------------+

Task 2 - Read from a csv file and write to parquet file
# Load student dataset
df = spark.read.csv("student-hw.csv", header=True, inferSchema=True)
​
# display dataframe
df.show()
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student7|           65|           80|
|student7|           65|           80|
|student2|           59|          100|
|student1|           64|           90|
|student3|           69|           95|
|student5|           60|           80|
|student3|           69|           95|
|student6|           62|           85|
|    null|           70|          110|
+--------+-------------+-------------+

# print the number of rows in the dataframe
df.count()
9
Drop Duplicates

df = df.dropDuplicates()
df.show()
[Stage 28:==============================>                         (41 + 8) / 75]
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student6|           62|           85|
|student3|           69|           95|
|student2|           59|          100|
|student7|           65|           80|
|    null|           70|          110|
|student1|           64|           90|
|student5|           60|           80|
+--------+-------------+-------------+

                                                                                
#Notice that the duplicates are removed
# print the number of rows in the dataframe
df.count()
                                                                                
7
Drop Null values

df=df.dropna()
#Observe the rows with null values getting dropped
df.show()
[Stage 41:=================================================>      (66 + 8) / 75]
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student6|           62|           85|
|student3|           69|           95|
|student2|           59|          100|
|student7|           65|           80|
|student1|           64|           90|
|student5|           60|           80|
+--------+-------------+-------------+

                                                                                
Save to parquet file

#Write the data to a Parquet file
df.write.mode("overwrite").parquet("student-hw.parquet")
                                                                                
# if you do not wish to overwrite use the command df.write.parquet("student-hw.parquet")
# verify that the parquet file(s) are created
!!ls -l student-hw.parquet
['total 7',
 '-rw-r--r-- 1 jupyterlab resources   0 Jul 21 00:23 _SUCCESS',
 '-rw-r--r-- 1 jupyterlab resources 467 Jul 21 00:23 part-00000-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00003-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00010-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00054-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00132-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00172-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet',
 '-rw-r--r-- 1 jupyterlab resources 911 Jul 21 00:23 part-00186-6226e508-acc2-4625-a2bf-99425daeee50-c000.snappy.parquet']
Notice that there are a lot of .parquet files in the output.

To improve parallellism, spark stores each dataframe in multiple partitions.
When the data is saved as parquet file, each partition is saved as a separate file.
Task 3 - Condense PARQUET to a single file.
Reduce the number of partitions in the dataframe to one.

df = df.repartition(1)
Save to parquet file

#Write the data to a Parquet file
df.write.mode("overwrite").parquet("student-hw-single.parquet")
                                                                                
# if you do not wish to overwrite use the command df.write.parquet("student-hw-single.parquet")
# verify that the parquet file(s) are created
!ls -l student-hw-single.parquet
total 2
-rw-r--r-- 1 jupyterlab resources   0 Jul 21 00:24 _SUCCESS
-rw-r--r-- 1 jupyterlab resources 944 Jul 21 00:24 part-00000-8e2a40aa-c64e-4947-b52d-59737eb0694a-c000.snappy.parquet
#Notice that there is only one .parquet file
Task 4 - Read from a parquet file and write to csv file
df = spark.read.parquet("student-hw-single.parquet")
df.show()
+--------+-------------+-------------+
| student|height_inches|weight_pounds|
+--------+-------------+-------------+
|student6|           62|           85|
|student3|           69|           95|
|student2|           59|          100|
|student7|           65|           80|
|student1|           64|           90|
|student5|           60|           80|
+--------+-------------+-------------+

Transform the data

#import the expr function that helps in transforming the data
from pyspark.sql.functions import expr
Convert inches to centimeters

# Convert inches to centimeters
# Multiply the column height_inches with 2.54 to get a new column height_centimeters
df = df.withColumn("height_centimeters", expr("height_inches * 2.54"))
df.show()
+--------+-------------+-------------+------------------+
| student|height_inches|weight_pounds|height_centimeters|
+--------+-------------+-------------+------------------+
|student6|           62|           85|            157.48|
|student3|           69|           95|            175.26|
|student2|           59|          100|            149.86|
|student7|           65|           80|            165.10|
|student1|           64|           90|            162.56|
|student5|           60|           80|            152.40|
+--------+-------------+-------------+------------------+

Convert pounds to kilograms

# Convert pounds to kilograms
# Multiply weight_pounds with 0.453592 to get a new column weight_kg
df = df.withColumn("weight_kg", expr("weight_pounds * 0.453592"))
df.show()
+--------+-------------+-------------+------------------+---------+
| student|height_inches|weight_pounds|height_centimeters|weight_kg|
+--------+-------------+-------------+------------------+---------+
|student6|           62|           85|            157.48|38.555320|
|student3|           69|           95|            175.26|43.091240|
|student2|           59|          100|            149.86|45.359200|
|student7|           65|           80|            165.10|36.287360|
|student1|           64|           90|            162.56|40.823280|
|student5|           60|           80|            152.40|36.287360|
+--------+-------------+-------------+------------------+---------+

Drop the columns

# drop the columns "height_inches","weight_pounds"
df = df.drop("height_inches","weight_pounds")
df.show()
+--------+------------------+---------+
| student|height_centimeters|weight_kg|
+--------+------------------+---------+
|student6|            157.48|38.555320|
|student3|            175.26|43.091240|
|student2|            149.86|45.359200|
|student7|            165.10|36.287360|
|student1|            162.56|40.823280|
|student5|            152.40|36.287360|
+--------+------------------+---------+

Rename a column

# rename the lengthy column name "height_centimeters" to "height_cm"
df = df.withColumnRenamed("height_centimeters","height_cm")
df.show()
+--------+---------+---------+
| student|height_cm|weight_kg|
+--------+---------+---------+
|student6|   157.48|38.555320|
|student3|   175.26|43.091240|
|student2|   149.86|45.359200|
|student7|   165.10|36.287360|
|student1|   162.56|40.823280|
|student5|   152.40|36.287360|
+--------+---------+---------+

Save to csv file

df.write.mode("overwrite").csv("student_transformed.csv", header=True)
                                                                                
Verify the csv file

# Load student dataset
df = spark.read.csv("student_transformed.csv", header=True, inferSchema=True)
# display dataframe
df.show()
+--------+---------+---------+
| student|height_cm|weight_kg|
+--------+---------+---------+
|student6|   157.48| 38.55532|
|student3|   175.26| 43.09124|
|student2|   149.86|  45.3592|
|student7|    165.1| 36.28736|
|student1|   162.56| 40.82328|
|student5|    152.4| 36.28736|
+--------+---------+---------+

Stop Spark Session

spark.stop()
Exercises
Create Spark Session

#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Exercises - ETL using Spark").getOrCreate()
Exercise 1 - Extract
Load data from student_transformed.csv into a dataframe


# Load student dataset
df = spark.read.csv("student_transformed.csv", header=True, inferSchema=True)
# display dataframe
df.show()
+--------+---------+---------+
| student|height_cm|weight_kg|
+--------+---------+---------+
|student6|   157.48| 38.55532|
|student3|   175.26| 43.09124|
|student2|   149.86|  45.3592|
|student7|    165.1| 36.28736|
|student1|   162.56| 40.82328|
|student5|    152.4| 36.28736|
+--------+---------+---------+


Exercise 2 - Transform
Convert cm to meters

.sql.functions
#import the expr function that helps in transforming the data
from pyspark.sql.functions import expr

# Convert centimeters to meters
# Divide the column height_cm by 100 a new column height_cm
df = df.withColumn("height_meters", expr("height_cm / 100"))
# display dataframe
df.show()
+--------+---------+---------+------------------+
| student|height_cm|weight_kg|     height_meters|
+--------+---------+---------+------------------+
|student6|   157.48| 38.55532|            1.5748|
|student3|   175.26| 43.09124|            1.7526|
|student2|   149.86|  45.3592|1.4986000000000002|
|student7|    165.1| 36.28736|             1.651|
|student1|   162.56| 40.82328|            1.6256|
|student5|    152.4| 36.28736|             1.524|
+--------+---------+---------+------------------+

Click here for a Hint
Click here for Solution
Create a column named bmi

# compute bmi using the below formula
# BMI = weight/(height * height)
# weight must be in kgs
# height must be in meters
df = df.withColumn("bmi", expr("weight_kg/(height_meters*height_meters)"))
# display dataframe
df.show()
+--------+---------+---------+------------------+------------------+
| student|height_cm|weight_kg|     height_meters|               bmi|
+--------+---------+---------+------------------+------------------+
|student6|   157.48| 38.55532|            1.5748|15.546531093062187|
|student3|   175.26| 43.09124|            1.7526|14.028892161964118|
|student2|   149.86|  45.3592|1.4986000000000002|20.197328530250278|
|student7|    165.1| 36.28736|             1.651|13.312549228648752|
|student1|   162.56| 40.82328|            1.6256|15.448293591899683|
|student5|    152.4| 36.28736|             1.524|15.623755691955827|
+--------+---------+---------+------------------+------------------+



# Drop the columns height_cm, weight_kg and height_meters
df = df.drop("height_cm","weight_kg","height_meters")
# display dataframe
df.show()
+--------+------------------+
| student|               bmi|
+--------+------------------+
|student6|15.546531093062187|
|student3|14.028892161964118|
|student2|20.197328530250278|
|student7|13.312549228648752|
|student1|15.448293591899683|
|student5|15.623755691955827|
+--------+------------------+


# Let us round the column bmi
from pyspark.sql.functions import col, round
df = df.withColumn("bmi_rounded", round(col("bmi")))
df.show()
+--------+------------------+-----------+
| student|               bmi|bmi_rounded|
+--------+------------------+-----------+
|student6|15.546531093062187|       16.0|
|student3|14.028892161964118|       14.0|
|student2|20.197328530250278|       20.0|
|student7|13.312549228648752|       13.0|
|student1|15.448293591899683|       15.0|
|student5|15.623755691955827|       16.0|
+--------+------------------+-----------+

Exercise 3 - Load
Save the dataframe into a parquet file

#Write the data to a Parquet file, set the mode to overwrite
df.write.mode("overwrite").parquet("student_transformed.parquet")
                                                                                

Stop Spark Session

spark.stop()
