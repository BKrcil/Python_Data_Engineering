Objectives
After completing this lab you will be able to:

Use PySpark to connect to a spark cluster.
Create a spark session.
Read a csv file into a data frame.
Split the dataset into training and testing sets.
Use VectorAssembler to combine multiple columns into a single vector column
Use Linear Regression to build a prediction model.
Use metrics to evaluate the model.
Stop the spark session
Datasets
In this lab you will be using dataset(s):

Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg
Modified version of diamonds dataset. Original dataset available at https://www.openml.org/search?type=data&sort=runs&id=42225&status=active
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
from pyspark.sql import SparkSession
​
#import functions/Classes for sparkml
​
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
​
# import functions/Classes for metrics
from pyspark.ml.evaluation import RegressionEvaluator
​
Task 1 - Create a spark session

#Ignore any warnings by SparkSession command
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Regressing using SparkML").getOrCreate()
24/07/20 01:23:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Task 2 - Load the data in a csv file into a dataframe
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
​
--2024-07-20 01:23:51--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13891 (14K) [text/csv]
Saving to: ‘mpg.csv.2’

mpg.csv.2           100%[===================>]  13.57K  --.-KB/s    in 0s      

2024-07-20 01:23:51 (46.2 MB/s) - ‘mpg.csv.2’ saved [13891/13891]

Load the dataset into the spark dataframe

# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.
​
# Load mpg dataset
mpg_data = spark.read.csv("mpg.csv", header=True, inferSchema=True)
​
                                                                                
Print the schema of the dataset

mpg_data.printSchema()
root
 |-- MPG: double (nullable = true)
 |-- Cylinders: integer (nullable = true)
 |-- Engine Disp: double (nullable = true)
 |-- Horsepower: integer (nullable = true)
 |-- Weight: integer (nullable = true)
 |-- Accelerate: double (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Origin: string (nullable = true)

show top 5 rows from the dataset

mpg_data.show(5)
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|
|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|
|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|
|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|
|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|
+----+---------+-----------+----------+------+----------+----+--------+
only showing top 5 rows

Task 3 - Identify the label column and the input columns
We ask the VectorAssembler to group a bunch of inputCols as single column named "features"

# Prepare feature vector
assembler = VectorAssembler(inputCols=["Cylinders", "Engine Disp", "Horsepower", "Weight", "Accelerate", "Year"], outputCol="features")
mpg_transformed_data = assembler.transform(mpg_data)
​
Display the assembled "features" and the label column "MPG"

mpg_transformed_data.select("features","MPG").show()
+--------------------+----+
|            features| MPG|
+--------------------+----+
|[8.0,390.0,190.0,...|15.0|
|[6.0,199.0,90.0,2...|21.0|
|[6.0,199.0,97.0,2...|18.0|
|[8.0,304.0,150.0,...|16.0|
|[8.0,455.0,225.0,...|14.0|
|[8.0,350.0,165.0,...|15.0|
|[8.0,307.0,130.0,...|18.0|
|[8.0,454.0,220.0,...|14.0|
|[8.0,400.0,150.0,...|15.0|
|[8.0,307.0,200.0,...|10.0|
|[8.0,383.0,170.0,...|15.0|
|[8.0,318.0,210.0,...|11.0|
|[8.0,360.0,215.0,...|10.0|
|[8.0,429.0,198.0,...|15.0|
|[6.0,200.0,85.0,2...|21.0|
|[8.0,302.0,140.0,...|17.0|
|[8.0,304.0,193.0,...| 9.0|
|[8.0,340.0,160.0,...|14.0|
|[6.0,198.0,95.0,2...|22.0|
|[8.0,440.0,215.0,...|14.0|
+--------------------+----+
only showing top 20 rows

Task 4 - Split the data
We split the data set in the ratio of 70:30. 70% training data, 30% testing data.

# Split data into training and testing sets
(training_data, testing_data) = mpg_transformed_data.randomSplit([0.7, 0.3], seed=42)
​
The random_state variable "seed" controls the shuffling applied to the data before applying the split. Pass the same integer for reproducible output across multiple function calls

Task 5 - Build and Train a Linear Regression Model
Create a LR model and train the model using the training data set

# Train linear regression model
# Ignore any warnings
​
lr = LinearRegression(featuresCol="features", labelCol="MPG")
model = lr.fit(training_data)
24/07/20 01:24:32 WARN util.Instrumentation: [4883257d] regParam is zero, which might cause numerical instability and overfitting.
24/07/20 01:24:32 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
24/07/20 01:24:32 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
24/07/20 01:24:32 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
24/07/20 01:24:32 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
Task 6 - Evaluate the model
Your model is now trained. We use the testing data to make predictions.

# Make predictions on testing data
predictions = model.transform(testing_data)
R Squared
#R-squared (R2): R2 is a statistical measure that represents the proportion of variance
#in the dependent variable (target) that is explained by the independent variables (features).
#Higher values indicate better performance.
​
evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)
print("R Squared =", r2)
​
R Squared = 0.7685170910359864
Root Mean Squared Error
#Root Mean Squared Error (RMSE): RMSE is the square root of the average of the squared differences
#between the predicted and actual values. It measures the average distance between the predicted
#and actual values, and lower values indicate better performance.
​
evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("RMSE =", rmse)
​
RMSE = 3.7870898850307
Mean Absolute Error
#Mean Absolute Error (MAE): MAE is the average of the absolute differences between the predicted and
#actual values. It measures the average absolute distance between the predicted and actual values, and
#lower values indicate better performance.
​
evaluator = RegressionEvaluator(labelCol="MPG", predictionCol="prediction", metricName="mae")
mae = evaluator.evaluate(predictions)
print("MAE =", mae)
​
MAE = 2.990444141294136
Stop Spark Session

spark.stop()


Exercises


Exercise 1 - Create a spark session
Create a spark session with appname "Diamond Price Prediction"

spark = SparkSession.builder.appName("Diamond Price Prediction").getOrCreate()

Exercise 2 - Load the data in a csv file into a dataframe
Download the data set

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
​
--2024-07-20 01:25:08--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3192561 (3.0M) [text/csv]
Saving to: ‘diamonds.csv.1’

diamonds.csv.1      100%[===================>]   3.04M  --.-KB/s    in 0.05s   

2024-07-20 01:25:08 (56.2 MB/s) - ‘diamonds.csv.1’ saved [3192561/3192561]

Load the dataset into a spark dataframe

diamond_data = spark.read.csv("diamonds.csv", header=True, inferSchema=True)
                                                                                

Display sample data from dataset

#TO DO
diamond_data.show(5)
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
|  s|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
|  1| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|
|  2| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|
|  3| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|
|  4| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|
|  5| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
only showing top 5 rows

Click here for Solution
Exercise 3 - Identify the label column and the input columns
use the price column as label column
use the columns carat,depth and table as features
Assemble the columns columns carat,depth and table into a single column named "features"



assembler = VectorAssembler(inputCols=["carat", "depth", "table"], outputCol="features")
diamond_transformed_data = assembler.transform(diamond_data)

Print the vectorized features and label columns

diamond_transformed_data.select("features","price").show()
+----------------+-----+
|        features|price|
+----------------+-----+
|[0.23,61.5,55.0]|  326|
|[0.21,59.8,61.0]|  326|
|[0.23,56.9,65.0]|  327|
|[0.29,62.4,58.0]|  334|
|[0.31,63.3,58.0]|  335|
|[0.24,62.8,57.0]|  336|
|[0.24,62.3,57.0]|  336|
|[0.26,61.9,55.0]|  337|
|[0.22,65.1,61.0]|  337|
|[0.23,59.4,61.0]|  338|
| [0.3,64.0,55.0]|  339|
|[0.23,62.8,56.0]|  340|
|[0.22,60.4,61.0]|  342|
|[0.31,62.2,54.0]|  344|
| [0.2,60.2,62.0]|  345|
|[0.32,60.9,58.0]|  345|
| [0.3,62.0,54.0]|  348|
| [0.3,63.4,54.0]|  351|
| [0.3,63.8,56.0]|  351|
| [0.3,62.7,59.0]|  351|
+----------------+-----+
only showing top 20 rows


Exercise 4 - Split the data
Split the dataset into training and testing sets in the ratio of 70:30.

(training_data, testing_data) = diamond_transformed_data.randomSplit([0.7, 0.3])

Exercise 5 - Build and Train a Linear Regression Model
Build a linear regression and train it


lr = LinearRegression(featuresCol="features", labelCol="price")
model = lr.fit(training_data)
​
24/07/20 01:26:18 WARN util.Instrumentation: [057ace0f] regParam is zero, which might cause numerical instability and overfitting.
                                                                                

Predict the values using the test data

Exercise 6 - Evaluate the model
Your model is now trained. Make the model predict on testing_data

predictions = model.transform(testing_data)
​

Print the metrics :

R squared
mean absolute error
root mean squared error






#your code goes here
evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)
print("R Squared =", r2)
​
​
evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName="mae")
mae = evaluator.evaluate(predictions)
print("MAE =", mae)
​
​
evaluator = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print("RMSE =", rmse)

R Squared = 0.8503651315878791                                                        
MAE = 998.8228731047611
RMSE = 1545.8973360177758
