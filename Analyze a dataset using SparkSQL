Objectives
After completing this lab you will be able to:

Load a csv dataset into a dataframe
Create a temporary view based on a dataframe
Run SQL queries on the view
Analyze a dataset using SparkSQL
Datasets
In this lab you will be using dataset(s):

Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg
Modified version of diamonds dataset. Original dataset available at https://www.openml.org/search?type=data&sort=runs&id=42225&status=active
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
#import functions/Classes for sparkml
​
from pyspark.sql import SparkSession
Examples
Task 1 - Create a spark session
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()
24/07/20 23:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Task 2 - Load csv file into a dataframe
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
​
--2024-07-20 23:46:45--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13891 (14K) [text/csv]
Saving to: ‘mpg.csv.3’

mpg.csv.3           100%[===================>]  13.57K  --.-KB/s    in 0s      

2024-07-20 23:46:46 (29.9 MB/s) - ‘mpg.csv.3’ saved [13891/13891]

Load the dataset into the spark dataframe

# Load mpg dataset
mpg_data = spark.read.csv("mpg.csv", header=True, inferSchema=True)
​
Task 3 - Create a temporary view
Create a temporary view of the DataFrame named mileage

​
mpg_data.createOrReplaceTempView("mileage")
​
Task 4 - Run a SQL query on the dataframe
Select all cars whose mileage is more than 40

results = spark.sql("SELECT * FROM mileage WHERE MPG > 40")
# show the results
results.show()
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|43.1|        4|       90.0|        48|  1985|      21.5|  78|European|
|43.4|        4|       90.0|        48|  2335|      23.7|  80|European|
|41.5|        4|       98.0|        76|  2144|      14.7|  80|European|
|44.3|        4|       90.0|        48|  2085|      21.7|  80|European|
|40.8|        4|       85.0|        65|  2110|      19.2|  80|Japanese|
|44.6|        4|       91.0|        67|  1850|      13.8|  80|Japanese|
|46.6|        4|       86.0|        65|  2110|      17.9|  80|Japanese|
|44.0|        4|       97.0|        52|  2130|      24.6|  82|European|
+----+---------+-----------+----------+------+----------+----+--------+

Task 5 - Analyze the dataset
List all the unique Origins

spark.sql("SELECT distinct Origin FROM mileage").show()
[Stage 12:================================>                      (44 + 10) / 75]
+--------+
|  Origin|
+--------+
|European|
|Japanese|
|American|
+--------+

                                                                                
Show the count of Japanese cars

spark.sql("SELECT count(*) FROM mileage where Origin ='Japanese' ").show()
+--------+
|count(1)|
+--------+
|      79|
+--------+

Count the number of cars with mileage greater than 40

spark.sql("SELECT count(*) FROM mileage where MPG > 40").show()
+--------+
|count(1)|
+--------+
|       8|
+--------+

List the number of cars made in different Years

spark.sql("SELECT Year, count(Year) FROM mileage group by Year").show()
[Stage 26:============================================>          (60 + 11) / 75]
+----+-----------+
|Year|count(Year)|
+----+-----------+
|  78|         36|
|  81|         28|
|  76|         34|
|  72|         28|
|  77|         28|
|  82|         30|
|  80|         27|
|  73|         40|
|  70|         29|
|  75|         30|
|  71|         27|
|  79|         29|
|  74|         26|
+----+-----------+

                                                                                
Print the maximum MPG

spark.sql("SELECT max(MPG) FROM mileage").show()
+--------+
|max(MPG)|
+--------+
|    46.6|
+--------+

Stop Spark Session

spark.stop()


Exercises


Exercise 1 - Create a spark session
Create a spark session with appname "SparkSQL Exercise"

spark = SparkSession.builder.appName("SparkSQL Exercise").getOrCreate()

Exercise 2 - Load csv file into a dataframe
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
​
--2024-07-20 23:48:02--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3192561 (3.0M) [text/csv]
Saving to: ‘diamonds.csv.2’

diamonds.csv.2      100%[===================>]   3.04M  --.-KB/s    in 0.06s   

2024-07-20 23:48:03 (47.0 MB/s) - ‘diamonds.csv.2’ saved [3192561/3192561]

Load the diamonds dataset into the spark dataframe

​
diamond_data = spark.read.csv("diamonds.csv", header=True, inferSchema=True)
​
                                                                                

Exercise 3 - Create a temporary view
Create a temporary view of the DataFrame named diamonds

#your code goes here
​
diamond_data.createOrReplaceTempView("diamonds")

Exercise 4 - Run a SQL query on the dataframe
Select all rows

results = spark.sql("SELECT * FROM diamonds")
results.show()
+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+
|  s|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|
+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+
|  1| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|
|  2| 0.21|  Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|
|  3| 0.23|     Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|
|  4| 0.29|  Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|
|  5| 0.31|     Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|
|  6| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|  336|3.94|3.96|2.48|
|  7| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|  336|3.95|3.98|2.47|
|  8| 0.26|Very Good|    H|    SI1| 61.9| 55.0|  337|4.07|4.11|2.53|
|  9| 0.22|     Fair|    E|    VS2| 65.1| 61.0|  337|3.87|3.78|2.49|
| 10| 0.23|Very Good|    H|    VS1| 59.4| 61.0|  338| 4.0|4.05|2.39|
| 11|  0.3|     Good|    J|    SI1| 64.0| 55.0|  339|4.25|4.28|2.73|
| 12| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|  340|3.93| 3.9|2.46|
| 13| 0.22|  Premium|    F|    SI1| 60.4| 61.0|  342|3.88|3.84|2.33|
| 14| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|  344|4.35|4.37|2.71|
| 15|  0.2|  Premium|    E|    SI2| 60.2| 62.0|  345|3.79|3.75|2.27|
| 16| 0.32|  Premium|    E|     I1| 60.9| 58.0|  345|4.38|4.42|2.68|
| 17|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|  348|4.31|4.34|2.68|
| 18|  0.3|     Good|    J|    SI1| 63.4| 54.0|  351|4.23|4.29| 2.7|
| 19|  0.3|     Good|    J|    SI1| 63.8| 56.0|  351|4.23|4.26|2.71|
| 20|  0.3|Very Good|    J|    SI1| 62.7| 59.0|  351|4.21|4.27|2.66|
+---+-----+---------+-----+-------+-----+-----+-----+----+----+----+
only showing top 20 rows

Exercise 5 - Analyze the dataset
List all the unique Cuts

#your code goes here
spark.sql("SELECT distinct cut FROM diamonds").show()
                                                                                
+---------+
|      cut|
+---------+
|  Premium|
|    Ideal|
|     Good|
|     Fair|
|Very Good|
+---------+

                                                                                

Show the count of Premium Cut diamonds

#your code goes here
spark.sql("SELECT count(*) FROM diamonds where cut ='Premium' ").show()
+--------+
|count(1)|
+--------+
|   13791|
+--------+


Count the number of diamonds with table size greater than 65

spark.sql("SELECT count(*) FROM diamonds where table > 65").show()
​
+--------+
|count(1)|
+--------+
|     181|
+--------+

List the number of diamonds under each color

#your code goes here
spark.sql("SELECT color, count(color) FROM diamonds group by color").show()
                                                                                
+-----+------------+
|color|count(color)|
+-----+------------+
|    F|        9542|
|    E|        9797|
|    D|        6775|
|    J|        2808|
|    G|       11292|
|    I|        5422|
|    H|        8304|
+-----+------------+

                                                                                

Print the maximum price

spark.sql("SELECT max(price) FROM diamonds").show()
+----------+
|max(price)|
+----------+
|     18823|
+----------+


Stop Spark Session

spark.stop()
