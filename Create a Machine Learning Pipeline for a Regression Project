Objectives
In this 4 part assignment you will:

Part 1 ETL
Load a csv dataset
Remove duplicates if any
Drop rows with null values if any
Make transformations
Store the cleaned data in parquet format
Part 2 Machine Learning Pipeline creation
Create a machine learning pipeline for prediction
Part 3 Model evaluation
Evaluate the model using metrics
Print the intercept and the coefficients
Part 4 Model Persistance
Cave the model for future production use
Load and verify the stored model
Datasets
In this lab you will be using dataset(s):

Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
Part 1 - ETL
Task 1 - Import required libraries
#your code goes here
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import StandardScaler
​

Task 2 - Create a spark session

#Create SparkSession
​
spark = SparkSession.builder.appName("Practice Project").getOrCreate()
​
24/07/22 21:41:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Click here for a Hint
Click here for Solution
Task 3 - Load the csv file into a dataframe
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv
​
--2024-07-22 21:41:58--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 14354 (14K) [text/csv]
Saving to: ‘mpg-raw.csv’

mpg-raw.csv         100%[===================>]  14.02K  --.-KB/s    in 0s      

2024-07-22 21:41:58 (44.5 MB/s) - ‘mpg-raw.csv’ saved [14354/14354]

Load the dataset into the spark dataframe

spark.read.csv("mpg-raw.csv", header=True, inferSchema=True)
# Load dataset
df = spark.read.csv("mpg-raw.csv", header=True, inferSchema=True)
​

Task 4 - Print top 5 rows of the dataset
#your code goes here
df.show(5)
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|46.6|        4|       86.0|        65|  2110|      17.9|  80|Japanese|
|44.6|        4|       91.0|        67|  1850|      13.8|  80|Japanese|
|44.3|        4|       90.0|        48|  2085|      21.7|  80|European|
|44.0|        4|       97.0|        52|  2130|      24.6|  82|European|
|43.4|        4|       90.0|        48|  2335|      23.7|  80|European|
+----+---------+-----------+----------+------+----------+----+--------+
only showing top 5 rows

Task 5 - Print the number of cars in each Origin
#your code goes here
df.groupBy('Origin').count().orderBy('count').show()
[Stage 3:===================================================>   (189 + 9) / 200]
+--------+-----+
|  Origin|count|
+--------+-----+
|    null|    1|
|European|   70|
|Japanese|   88|
|American|  247|
+--------+-----+

                                                                                

df.groupBy('Origin').count().orderBy('count').show()
Task 6 - Print the total number of rows in the dataset
df.count()
#your code goes here
rowcount1 = df.count()
print(rowcount1)
406
Task 7 - Drop all the duplicate rows from the dataset
#your code goes here
df = df.dropDuplicates()

Task 8 - Print the total number of rows in the dataset
#TODO
#your code goes here
rowcount2 = df.count()
print(rowcount2)
[Stage 7:===================================================>   (186 + 9) / 200]
392
                                                                                
Task 9 - Drop all the rows that contain null values from the dataset
#your code goes here
df=df.dropna()

Task 10 - Print the total number of rows in the dataset
df.count()
#your code goes here
rowcount3 = df.count()
print(rowcount3)
[Stage 10:=================================================>    (185 + 8) / 200]
385
                                                                                
Task 11 - Rename the column "Engine Disp" to "Engine_Disp"Drop

#your code goes here
​
df = df.withColumnRenamed("Engine Disp","Engine_Disp")

Task 12 - Save the dataframe in parquet format, name the file as "mpg-cleaned.parquet"
#your code goes here
df.write.mode("overwrite").parquet("mpg-cleaned.parquet")
[Stage 13:=======>                                               (28 + 8) / 200]24/07/22 21:45:02 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory
Scaling row group sizes to 96.54% for 7 writers
                                                                                

Part 1 - Evaluation
Run the code cell below.
If the code throws up any errors, go back and review the code you have written.

print("Part 1 - Evaluation")
​
print("Total rows = ", rowcount1)
print("Total rows after dropping duplicate rows = ", rowcount2)
print("Total rows after dropping duplicate rows and rows with null values = ", rowcount3)
print("Renamed column name = ", df.columns[2])
​
import os
​
print("mpg-cleaned.parquet exists :", os.path.isdir("mpg-cleaned.parquet"))
Part 1 - Evaluation
Total rows =  406
Total rows after dropping duplicate rows =  392
Total rows after dropping duplicate rows and rows with null values =  385
Renamed column name =  Engine Disp
mpg-cleaned.parquet exists : True
Part - 2 Machine Learning Pipeline creation
Task 1 - Load data from "mpg-cleaned.parquet" into a dataframe
#your code goes here
​
df = spark.read.parquet("mpg-cleaned.parquet")
rowcount4 = df.count()
                                                                                
#show top 5 rows
#your code goes here
df.show(5)
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine_Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|32.2|        4|      108.0|        75|  2265|      15.2|  80|Japanese|
|28.0|        4|      107.0|        86|  2464|      15.5|  76|European|
|26.0|        4|      156.0|        92|  2585|      14.5|  82|American|
|25.0|        4|      104.0|        95|  2375|      17.5|  70|European|
|25.0|        4|      140.0|        75|  2542|      17.0|  74|American|
+----+---------+-----------+----------+------+----------+----+--------+
only showing top 5 rows

#print the schema of the dataframe
#your code goes here
df.printSchema()
root
 |-- MPG: double (nullable = true)
 |-- Cylinders: integer (nullable = true)
 |-- Engine_Disp: double (nullable = true)
 |-- Horsepower: integer (nullable = true)
 |-- Weight: integer (nullable = true)
 |-- Accelerate: double (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Origin: string (nullable = true)

Task 2 - Define the StringIndexer pipeline stage

# Stage - 1 Using StringIndexer convert the string column "Origin" into "OriginIndex"
indexer = StringIndexer(inputCol="Origin", outputCol="OriginIndex")

Task 3 - Define the VectorAssembler pipeline stage

# Stage 2 - assemble the input columns 'Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year' into a single column "features"
assembler = VectorAssembler(inputCols=['Cylinders','Engine_Disp','Horsepower','Weight','Accelerate','Year'], outputCol="features")

Task 4 - Define the StandardScaler pipeline stage

# Stage 3 - scale the "features" using standard scaler and store in "scaledFeatures" column
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

Task 5 - Define the Model creation pipeline stage

# Stage 4 - Create a LinearRegression stage to predict "MPG"
​
lr = LinearRegression(featuresCol="scaledFeatures", labelCol="MPG")

Task 6 - Build the pipeline

# Build a pipeline using the above four stages
​
pipeline = Pipeline(stages=[indexer,assembler, scaler, lr])

Task 7 - Split the data

# Split the data into training and testing sets with 70:30 split. Use 42 as seed
(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)

Task 8 - Fit the pipeline

# Fit the pipeline using the training data
​
pipelineModel = pipeline.fit(trainingData)
24/07/22 21:52:38 WARN util.Instrumentation: [aadc637e] regParam is zero, which might cause numerical instability and overfitting.
[Stage 30:>                                                         (0 + 8) / 8]24/07/22 21:52:41 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
24/07/22 21:52:41 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
24/07/22 21:52:41 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
24/07/22 21:52:41 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
                                                                                

Part 2 - Evaluation
Run the code cell below.
If the code throws up any errors, go back and review the code you have written.

print("Part 2 - Evaluation")
print("Total rows = ", rowcount4)
ps = [str(x).split("_")[0] for x in pipeline.getStages()]
​
print("Pipeline Stage 1 = ", ps[0])
print("Pipeline Stage 2 = ", ps[1])
print("Pipeline Stage 3 = ", ps[2])
​
print("Label column = ", lr.getLabelCol())
Part 2 - Evaluation
Total rows =  385
Pipeline Stage 1 =  StringIndexer
Pipeline Stage 2 =  VectorAssembler
Pipeline Stage 3 =  StandardScaler
Label column =  MPG
Part 3 - Model Evaluation
Task 1 - Predict using the model

# Make predictions on testing data
predictions = pipelineModel.transform(testingData)

#Your code goes here
​
from pyspark.ml.evaluation import RegressionEvaluator
​
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="mse")
mse = evaluator.evaluate(predictions)
print(mse)
​
[Stage 37:=====================>                                    (3 + 5) / 8]
10.899168410636614
                                                                                
Click here for a Hint
Click here for Solution
Task 3 - Print the MAE

#Your code goes here
​
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="mae")
mae = evaluator.evaluate(predictions)
print(mae)
​
​
[Stage 39:==============>                                           (2 + 6) / 8]
2.625684006095087
                                                                                

Task 4 - Print the R-Squared(R2)

#Your code goes here
​
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="MPG", metricName="r2")
r2 = evaluator.evaluate(predictions)
print(r2)
​
[Stage 41:=======>                                                  (1 + 7) / 8]
0.8202762026606879
                                                                                
Click here for a Hint
Click here for Solution
Part 3 - Evaluation
Run the code cell below.
If the code throws up any errors, go back and review the code you have written.

print("Part 3 - Evaluation")
​
print("Mean Squared Error = ", round(mse,2))
print("Mean Absolute Error = ", round(mae,2))
print("R Squared = ", round(r2,2))
​
lrModel = pipelineModel.stages[-1]
​
print("Intercept = ", round(lrModel.intercept,2))
​
Part 3 - Evaluation
Mean Squared Error =  10.9
Mean Absolute Error =  2.63
R Squared =  0.82
Intercept =  -13.9
Part 4 - Model persistance
Task 1 - Save the model to the path "Practice_Project"
# Save the pipeline model
# your code goes here
pipelineModel.write().save("Practice_Project")
                                                                                

pipelineModel.write().save("Practice_Project")
Task 2 - Load the model from the path "Practice_Project"
# Load the pipeline model
# your code goes here
loadedPipelineModel = PipelineModel.load("Practice_Project")
                                                                                

Task 3 - Make predictions using the loaded model on the test data
# Use the loaded pipeline model for predictions
# your code goes here
predictions = loadedPipelineModel.transform(testingData)

Task 4 - Show the predictions
# your code goes here
predictions.select("MPG","prediction").show()
[Stage 70:>                                                         (0 + 1) / 1]
+----+------------------+
| MPG|        prediction|
+----+------------------+
|13.0| 11.31880991495555|
|13.0|14.368910120039454|
|13.0|10.684980370654229|
|15.0| 13.06659233990255|
|15.5|15.669787158219199|
|18.0| 19.81597780531922|
|18.0|22.299804385989376|
|18.0|20.053788782604926|
|18.6|20.890856761279736|
|19.0|24.862180440385856|
|21.5|26.265038242693542|
|22.0| 23.09860601340658|
|23.0| 21.27831362520896|
|24.0|22.969583765345746|
|25.1|27.030283144018128|
|26.0|27.950969788034072|
|26.0|25.754412517142313|
|27.0| 28.40010908515616|
|29.0|28.123745554548947|
|30.0| 30.24946672093104|
+----+------------------+
only showing top 20 rows

                                                                                

Part 4 - Evaluation
Run the code cell below.
If the code throws up any errors, go back and review the code you have written.

print("Part 4 - Evaluation")
​
loadedmodel = loadedPipelineModel.stages[-1]
totalstages = len(loadedPipelineModel.stages)
inputcolumns = loadedPipelineModel.stages[1].getInputCols()
​
print("Number of stages in the pipeline = ", totalstages)
for i,j in zip(inputcolumns, loadedmodel.coefficients):
    print(f"Coefficient for {i} is {round(j,4)}")
Part 4 - Evaluation
Number of stages in the pipeline =  4
Coefficient for Cylinders is -1.1901
Coefficient for Engine_Disp is 1.0404
Coefficient for Horsepower is -0.0605
Coefficient for Weight is -5.3403
Coefficient for Accelerate is 0.1052
Coefficient for Year is 2.7429
Task 5 - Stop Spark Session
spark.stop()
