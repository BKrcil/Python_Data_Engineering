Objectives¶
After completing this lab you will be able to:

Save a trained model.
Load a saved model.
Make predictions using the loaded model.
Datasets
In this lab you will be using dataset(s):

Modified version of car mileage dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/auto+mpg
Modified version of diamonds dataset. Original dataset available at https://www.openml.org/search?type=data&sort=runs&id=42225&status=active
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
#import functions/Classes for sparkml
​
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
​
# import functions/Classes for metrics
from pyspark.ml.evaluation import RegressionEvaluator
​
Examples
Task 1 - Create a model
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Model Persistence").getOrCreate()
24/07/22 03:39:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
​
--2024-07-22 03:39:30--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 13891 (14K) [text/csv]
Saving to: ‘mpg.csv.6’

mpg.csv.6           100%[===================>]  13.57K  --.-KB/s    in 0s      

2024-07-22 03:39:30 (46.1 MB/s) - ‘mpg.csv.6’ saved [13891/13891]

Load the dataset into the spark dataframe

# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.
​
# Load mpg dataset
mpg_data = spark.read.csv("mpg.csv", header=True, inferSchema=True)
​
Print the schema of the dataset

mpg_data.printSchema()
root
 |-- MPG: double (nullable = true)
 |-- Cylinders: integer (nullable = true)
 |-- Engine Disp: double (nullable = true)
 |-- Horsepower: integer (nullable = true)
 |-- Weight: integer (nullable = true)
 |-- Accelerate: double (nullable = true)
 |-- Year: integer (nullable = true)
 |-- Origin: string (nullable = true)

Show top 5 rows from the dataset

mpg_data.show(5)
+----+---------+-----------+----------+------+----------+----+--------+
| MPG|Cylinders|Engine Disp|Horsepower|Weight|Accelerate|Year|  Origin|
+----+---------+-----------+----------+------+----------+----+--------+
|15.0|        8|      390.0|       190|  3850|       8.5|  70|American|
|21.0|        6|      199.0|        90|  2648|      15.0|  70|American|
|18.0|        6|      199.0|        97|  2774|      15.5|  70|American|
|16.0|        8|      304.0|       150|  3433|      12.0|  70|American|
|14.0|        8|      455.0|       225|  3086|      10.0|  70|American|
+----+---------+-----------+----------+------+----------+----+--------+
only showing top 5 rows

We ask the VectorAssembler to group a bunch of inputCols as single column named "features"

# Prepare feature vector
assembler = VectorAssembler(inputCols=["Cylinders", "Engine Disp", "Horsepower", "Weight", "Accelerate", "Year"], outputCol="features")
mpg_transformed_data = assembler.transform(mpg_data)
​
Display the assembled "features" and the label column "MPG"

mpg_transformed_data.select("features","MPG").show()
+--------------------+----+
|            features| MPG|
+--------------------+----+
|[8.0,390.0,190.0,...|15.0|
|[6.0,199.0,90.0,2...|21.0|
|[6.0,199.0,97.0,2...|18.0|
|[8.0,304.0,150.0,...|16.0|
|[8.0,455.0,225.0,...|14.0|
|[8.0,350.0,165.0,...|15.0|
|[8.0,307.0,130.0,...|18.0|
|[8.0,454.0,220.0,...|14.0|
|[8.0,400.0,150.0,...|15.0|
|[8.0,307.0,200.0,...|10.0|
|[8.0,383.0,170.0,...|15.0|
|[8.0,318.0,210.0,...|11.0|
|[8.0,360.0,215.0,...|10.0|
|[8.0,429.0,198.0,...|15.0|
|[6.0,200.0,85.0,2...|21.0|
|[8.0,302.0,140.0,...|17.0|
|[8.0,304.0,193.0,...| 9.0|
|[8.0,340.0,160.0,...|14.0|
|[6.0,198.0,95.0,2...|22.0|
|[8.0,440.0,215.0,...|14.0|
+--------------------+----+
only showing top 20 rows

We split the data set in the ratio of 70:30. 70% training data, 30% testing data.

# Split data into training and testing sets
(training_data, testing_data) = mpg_transformed_data.randomSplit([0.7, 0.3])
​
Create a LR model and train the model using the pipeline on training data set

# Train linear regression model
# Ignore any warnings
lr = LinearRegression(labelCol="MPG", featuresCol="features")
pipeline = Pipeline(stages=[lr])
model = pipeline.fit(training_data)
​
24/07/22 03:40:00 WARN util.Instrumentation: [ded6bcf0] regParam is zero, which might cause numerical instability and overfitting.
24/07/22 03:40:01 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
24/07/22 03:40:01 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
24/07/22 03:40:01 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK
24/07/22 03:40:01 WARN netlib.LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK
Task 2 - Save the model
Create a folder where the model will to be saved

!mkdir model_storage
# Persist the model to the path "./model_stoarage/"
​
model.write().overwrite().save("./model_storage/")
​
#The overwrite method is used to overwrite the model if it already exists,
#and the save method is used to specify the path where the model should be saved.
​
​
                                                                                
Task 3 - Load the model
from pyspark.ml.pipeline import PipelineModel
​
# Load persisted model
loaded_model = PipelineModel.load("./model_storage/")
Task 4 - Predict using the loaded model
# Make predictions on test data
predictions = loaded_model.transform(testing_data)
#In the above example, we use the load method of the PipelineModel object to load the persisted model from disk. We can then use this loaded model to make predictions on new data using the transform method.
​
Your model is now trained. We use the testing data to make predictions.

# Make predictions on testing data
predictions = model.transform(testing_data)
predictions.select("prediction").show(5)
+------------------+
|        prediction|
+------------------+
| 16.62162778677613|
|17.523895439855366|
| 12.68230247272733|
|16.928697599953317|
| 11.66956499514366|
+------------------+
only showing top 5 rows

Stop Spark Session

spark.stop()
Exercises
Exercise 1 - Create a model
Create a spark session with appname "Model Persistence Exercise"

spark = SparkSession.builder.appName("Model Persistence Exercise").getOrCreate()

Download the data set

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
​
--2024-07-22 03:41:07--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/diamonds.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 3192561 (3.0M) [text/csv]
Saving to: ‘diamonds.csv.3’

diamonds.csv.3      100%[===================>]   3.04M  --.-KB/s    in 0.06s   

2024-07-22 03:41:08 (48.5 MB/s) - ‘diamonds.csv.3’ saved [3192561/3192561]

Load the dataset into a spark dataframe

diamond_data = spark.read.csv("diamonds.csv", header=True, inferSchema=True)
                                                                                

Display sample data from dataset

diamond_data.show(5)
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
|  s|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
|  1| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|
|  2| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|
|  3| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|
|  4| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|
|  5| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|
+---+-----+-------+-----+-------+-----+-----+-----+----+----+----+
only showing top 5 rows

Assemble the columns columns carat,depth and table into a single column named "features"


assembler = VectorAssembler(inputCols=["carat", "depth", "table"], outputCol="features")
diamond_transformed_data = assembler.transform(diamond_data)

Print the vectorized features and label columns

diamond_transformed_data.select("features","price").show()
+----------------+-----+
|        features|price|
+----------------+-----+
|[0.23,61.5,55.0]|  326|
|[0.21,59.8,61.0]|  326|
|[0.23,56.9,65.0]|  327|
|[0.29,62.4,58.0]|  334|
|[0.31,63.3,58.0]|  335|
|[0.24,62.8,57.0]|  336|
|[0.24,62.3,57.0]|  336|
|[0.26,61.9,55.0]|  337|
|[0.22,65.1,61.0]|  337|
|[0.23,59.4,61.0]|  338|
| [0.3,64.0,55.0]|  339|
|[0.23,62.8,56.0]|  340|
|[0.22,60.4,61.0]|  342|
|[0.31,62.2,54.0]|  344|
| [0.2,60.2,62.0]|  345|
|[0.32,60.9,58.0]|  345|
| [0.3,62.0,54.0]|  348|
| [0.3,63.4,54.0]|  351|
| [0.3,63.8,56.0]|  351|
| [0.3,62.7,59.0]|  351|
+----------------+-----+
only showing top 20 rows

Split the dataset into training and testing sets in the ratio of 70:30.

(training_data, testing_data) = diamond_transformed_data.randomSplit([0.7, 0.3])

Create a LR model and train the model using the pipeline on training data set



# Train linear regression model
# Ignore any warnings
lr = LinearRegression(labelCol="price", featuresCol="features")
pipeline = Pipeline(stages=[lr])
model = pipeline.fit(training_data)
24/07/22 03:41:53 WARN util.Instrumentation: [6a7de156] regParam is zero, which might cause numerical instability and overfitting.
                                                                                

Exercise 2 - Save the model
Create a folder "diamond_model". This is where the model will to be saved

!mkdir diamond_model
Persist the model to the folder "diamond_model"

#your code goes here
model.write().overwrite().save("./diamond_model/")
                                                                                

Exercise 3 - Load the model
Load the model from the folder "diamond_model"


from pyspark.ml.pipeline import PipelineModel
​
# Load persisted model
loaded_model = PipelineModel.load("./diamond_model/")

Exercise 4 - Predict using the loaded model
Make predictions on test data

​
predictions = loaded_model.transform(testing_data)
​

predictions.select("prediction").show(5)
+-------------------+
|         prediction|
+-------------------+
| -244.7842080975788|
|-1469.7595747883133|
| 385.72409213318315|
|-1014.9371876955556|
| -60.43935287192107|
+-------------------+
only showing top 5 rows

                                                                                
Stop Spark Session

spark.stop()






