Final Project - Build an ML Pipeline for Airfoil noise prediction
Estimated time needed: 90 minutes

Scenario
You are a data engineer at an aeronautics consulting company. Your company prides itself in being able to efficiently design airfoils for use in planes and sports cars. Data scientists in your office need to work with different algorithms and data in different formats. While they are good at Machine Learning, they count on you to be able to do ETL jobs and build ML pipelines. In this project you will use the modified version of the NASA Airfoil Self Noise dataset. You will clean this dataset, by dropping the duplicate rows, and removing the rows with null values. You will create an ML pipe line to create a model that will predict the SoundLevel based on all the other columns. You will evaluate the model and towards the end you will persist the model.

Objectives
In this 4 part assignment you will:

Part 1 Perform ETL activity
Load a csv dataset
Remove duplicates if any
Drop rows with null values if any
Make transformations
Store the cleaned data in parquet format
Part 2 Create a Machine Learning Pipeline
Create a machine learning pipeline for prediction
Part 3 Evaluate the Model
Evaluate the model using relevant metrics
Part 4 Persist the Model
Save the model for future production use
Load and verify the stored model
Datasets
In this lab you will be using dataset(s):

The original dataset can be found here NASA airfoil self noise dataset. https://archive.ics.uci.edu/dataset/291/airfoil+self+noise

This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license.

Diagram of an airfoil. - For informational purpose

Airfoil with flow

Diagram showing the Angle of attack. - For informational purpose

Airfoil angle of attack

Before you Start
Before you start attempting this project it is highly recommended that you finish the practice project.

Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
Part 1 - Perform ETL activity
Task 1 - Import required libraries
#your code goes here
​
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.pipeline import PipelineModel
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import StandardScaler
Task 2 - Create a spark session
#Create a SparkSession
​
spark = SparkSession.builder.appName("Final Project").getOrCreate()
Task 3 - Load the csv file into a dataframe
Download the data file.

NOTE : Please ensure you use the dataset below and not the original dataset mentioned above.

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv
​
--2024-07-22 22:48:44--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/NASA_airfoil_noise_raw.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 60682 (59K) [text/csv]
Saving to: ‘NASA_airfoil_noise_raw.csv.1’

NASA_airfoil_noise_ 100%[===================>]  59.26K  --.-KB/s    in 0.002s  

2024-07-22 22:48:44 (32.4 MB/s) - ‘NASA_airfoil_noise_raw.csv.1’ saved [60682/60682]

Load the dataset into the spark dataframe

# Load the dataset that you have downloaded in the previous task
​
df = spark.read.csv("NASA_airfoil_noise_raw.csv", header=True, inferSchema=True)
​
Task 4 - Print top 5 rows of the dataset
#your code goes here
df.show(5)
+---------+-------------+-----------+------------------+-----------------------+----------+
|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevel|
+---------+-------------+-----------+------------------+-----------------------+----------+
|      800|          0.0|     0.3048|              71.3|             0.00266337|   126.201|
|     1000|          0.0|     0.3048|              71.3|             0.00266337|   125.201|
|     1250|          0.0|     0.3048|              71.3|             0.00266337|   125.951|
|     1600|          0.0|     0.3048|              71.3|             0.00266337|   127.591|
|     2000|          0.0|     0.3048|              71.3|             0.00266337|   127.461|
+---------+-------------+-----------+------------------+-----------------------+----------+
only showing top 5 rows

Task 6 - Print the total number of rows in the dataset
#your code goes here
rowcount1 = df.count()
print(rowcount1)
1522
Task 7 - Drop all the duplicate rows from the dataset
df = df.dropDuplicates()
​
Task 8 - Print the total number of rows in the dataset
#your code goes here
​
rowcount2 = df.count()
print(rowcount2)
​
[Stage 6:================================================>      (177 + 8) / 200]
1503
                                                                                
Task 9 - Drop all the rows that contain null values from the dataset
df=df.dropna()
​
Task 10 - Print the total number of rows in the dataset
#your code goes here
​
rowcount3 = df.count()
print(rowcount3)
​
[Stage 9:=====================================================> (194 + 6) / 200]
1499
                                                                                
Task 11 - Rename the column "SoundLevel" to "SoundLevelDecibels"
# your code goes here
​
df = df.withColumnRenamed("SoundLevel","SoundLevelDecibels")
​
Task 12 - Save the dataframe in parquet format, name the file as "NASA_airfoil_noise_cleaned.parquet"
# your code goes here
df.write.mode("overwrite").parquet("NASA_airfoil_noise_cleaned.parquet")
​
[Stage 12:>                                                       (0 + 8) / 200]24/07/22 22:09:35 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory
Scaling row group sizes to 96.54% for 7 writers
24/07/22 22:09:35 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory
Scaling row group sizes to 84.47% for 8 writers
24/07/22 22:09:35 WARN hadoop.MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory
Scaling row group sizes to 96.54% for 7 writers
                                                                                
Part 1 - Evaluation
Run the code cell below.
Use the answers here to answer the final evaluation quiz in the next section.
If the code throws up any errors, go back and review the code you have written.

print("Part 1 - Evaluation")
​
print("Total rows = ", rowcount1)
print("Total rows after dropping duplicate rows = ", rowcount2)
print("Total rows after dropping duplicate rows and rows with null values = ", rowcount3)
print("New column name = ", df.columns[-1])
​
import os
​
print("NASA_airfoil_noise_cleaned.parquet exists :", os.path.isdir("NASA_airfoil_noise_cleaned.parquet"))
Part 1 - Evaluation
Total rows =  1522
Total rows after dropping duplicate rows =  1503
Total rows after dropping duplicate rows and rows with null values =  1499
New column name =  SoundLevel
NASA_airfoil_noise_cleaned.parquet exists : True
Part - 2 Create a Machine Learning Pipeline
Task 1 - Load data from "NASA_airfoil_noise_cleaned.parquet" into a dataframe
#your code goes here
​
df = spark.read.parquet("NASA_airfoil_noise_cleaned.parquet")
​
Task 2 - Print the total number of rows in the dataset
#your code goes here
​
rowcount4 = df.count()
print(rowcount4)
​
​
[Stage 12:====================================>                     (5 + 3) / 8]
1499
                                                                                
Task 3 - Define the VectorAssembler pipeline stage
Stage 1 - Assemble the input columns into a single column "features". Use all the columns except SoundLevelDecibels as input features.

#your code goes here
df.printSchema()
assembler = VectorAssembler(inputCols=['Frequency','AngleOfAttack','ChordLength','FreeStreamVelocity','SuctionSideDisplacement'], outputCol="features")
​
​
root
 |-- Frequency: integer (nullable = true)
 |-- AngleOfAttack: double (nullable = true)
 |-- ChordLength: double (nullable = true)
 |-- FreeStreamVelocity: double (nullable = true)
 |-- SuctionSideDisplacement: double (nullable = true)
 |-- SoundLevelDecibels: double (nullable = true)

Task 4 - Define the StandardScaler pipeline stage
Stage 2 - Scale the "features" using standard scaler and store in "scaledFeatures" column

"features"
#your code goes here
​
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
​
Task 5 - Define the Model creation pipeline stage
Stage 3 - Create a LinearRegression stage to predict "SoundLevelDecibels"

Note:You need to use the scaledfeatures retreived in the previous step(StandardScaler pipeline stage).

lr = LinearRegression(featuresCol="scaledFeatures", labelCol="Frequency")

#your code goes here
​
lr = LinearRegression(featuresCol='scaledFeatures', labelCol='SoundLevelDecibels', predictionCol='prediction')
Task 6 - Build the pipeline
Build a pipeline using the above three stages

#your code goes here
​
pipeline = Pipeline(stages=[assembler,scaler, lr])
Task 7 - Split the data
# Split the data into training and testing sets with 70:30 split.
# set the value of seed to 42
# the above step is very important. DO NOT set the value of seed to any other value other than 42.
​
#your code goes here
​
(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)
​
​
Task 8 - Fit the pipeline
# Fit the pipeline using the training data
# your code goes here
​
pipelineModel = pipeline.fit(trainingData)
​
24/07/22 22:54:04 WARN util.Instrumentation: [425141e4] regParam is zero, which might cause numerical instability and overfitting.
                                                                                
Part 2 - Evaluation
Run the code cell below.
Use the answers here to answer the final evaluation quiz in the next section.
If the code throws up any errors, go back and review the code you have written.

print("Part 2 - Evaluation")
print("Total rows = ", rowcount4)
ps = [str(x).split("_")[0] for x in pipeline.getStages()]
​
print("Pipeline Stage 1 = ", ps[0])
print("Pipeline Stage 2 = ", ps[1])
print("Pipeline Stage 3 = ", ps[2])
​
print("Label column = ", lr.getLabelCol())
Part 2 - Evaluation
Total rows =  1499
Pipeline Stage 1 =  VectorAssembler
Pipeline Stage 2 =  StandardScaler
Pipeline Stage 3 =  LinearRegression
Label column =  SoundLevelDecibels
Part 3 - Evaluate the Model
Task 1 - Predict using the model
# Make predictions on testing data
# your code goes here
​
predictions = pipelineModel.transform(testingData)
​
Task 2 - Print the MSE
#your code goes here
​
from pyspark.ml.evaluation import RegressionEvaluator
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="mse")
mse = evaluator.evaluate(predictions)
print(mse)
​
[Stage 24:=============================>                            (4 + 4) / 8]
22.593754071348812
                                                                                
Task 3 - Print the MAE
Frequency
#your code goes here
​
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="mae")
mae = evaluator.evaluate(predictions)
print(mae)
​
[Stage 26:>                                                         (0 + 8) / 8]
3.7336902294631287
                                                                                
Task 4 - Print the R-Squared(R2)
#your code goes here
​
evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="SoundLevelDecibels", metricName="r2")
r2 = evaluator.evaluate(predictions)
print(r2)
​
[Stage 30:==============>                                           (2 + 6) / 8]
0.5426016508689058
                                                                                
Part 3 - Evaluation
Run the code cell below.
Use the answers here to answer the final evaluation quiz in the next section.
If the code throws up any errors, go back and review the code you have written.

print("Part 3 - Evaluation")
​
print("Mean Squared Error = ", round(mse,2))
print("Mean Absolute Error = ", round(mae,2))
print("R Squared = ", round(r2,2))
​
lrModel = pipelineModel.stages[-1]
​
print("Intercept = ", round(lrModel.intercept,2))
​
Part 3 - Evaluation
Mean Squared Error =  22.59
Mean Absolute Error =  3.73
R Squared =  0.54
Intercept =  132.6
Part 4 - Persist the Model
Task 1 - Save the model to the path "Final_Project"
()
# Save the pipeline model as "Final_Project"
# your code goes here
pipelineModel.write().overwrite().save("Final_Project")
​
                                                                                
Task 2 - Load the model from the path "Final_Project"
# Load the pipeline model you have created in the previous step
loadedPipelineModel = PipelineModel.load("Final_Project")
Task 3 - Make predictions using the loaded model on the testdata
# Use the loaded pipeline model and make predictions using testingData
predictions = loadedPipelineModel.transform(testingData)
​
Task 4 - Show the predictions
5
#show top 5 rows from the predections dataframe. Display only the label column and predictions
#your code goes here
predictions.select("SoundLevelDecibels","prediction").show(5)
[Stage 65:>                                                         (0 + 1) / 1]
+------------------+------------------+
|SoundLevelDecibels|        prediction|
+------------------+------------------+
|           127.315|123.64344009624753|
|           119.975|123.48695788614877|
|           121.783|124.38983849684254|
|           127.224|121.44706993294302|
|           122.229|125.68312652454188|
+------------------+------------------+
only showing top 5 rows

                                                                                
Part 4 - Evaluation
Run the code cell below.
Use the answers here to answer the final evaluation quiz in the next section.
If the code throws up any errors, go back and review the code you have written.

print("Part 4 - Evaluation")
​
loadedmodel = loadedPipelineModel.stages[-1]
totalstages = len(loadedPipelineModel.stages)
inputcolumns = loadedPipelineModel.stages[0].getInputCols()
​
print("Number of stages in the pipeline = ", totalstages)
for i,j in zip(inputcolumns, loadedmodel.coefficients):
    print(f"Coefficient for {i} is {round(j,4)}")
Part 4 - Evaluation
Number of stages in the pipeline =  3
Coefficient for Frequency is -3.9728
Coefficient for AngleOfAttack is -2.4775
Coefficient for ChordLength is -3.3818
Coefficient for FreeStreamVelocity is 1.5789
Coefficient for SuctionSideDisplacement is -1.6465
Stop Spark Session
spark.stop()
