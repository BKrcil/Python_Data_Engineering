Objectives
After completing this lab you will be able to:

Use PySpark to connect to a spark cluster.
Create a spark session.
Read a csv file into a data frame.
Use KMeans algorithm to cluster the data
Stop the spark session
Datasets
In this lab you will be using dataset(s):

Modified version of Wholesale customers dataset. Original dataset available at https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
Seeds dataset. Available at https://archive.ics.uci.edu/ml/datasets/seeds
Setup
For this lab, we will be using the following libraries:

PySpark for connecting to the Spark Cluster
Installing Required Libraries
Spark Cluster is pre-installed in the Skills Network Labs environment. However, you need libraries like pyspark and findspark to connect to this cluster.

If you wish to download this jupyter notebook and run on your local computer, follow the instructions mentioned here.

The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:

!pip install pyspark==3.1.2 -q
!pip install findspark -q
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
​
# FindSpark simplifies the process of using Apache Spark with Python
​
import findspark
findspark.init()
​
#import functions/Classes for sparkml
​
from pyspark.ml.clustering import KMeans
from pyspark.ml.feature import VectorAssembler
​
from pyspark.sql import SparkSession
​
Examples
Task 1 - Create a spark session
#Create SparkSession
#Ignore any warnings by SparkSession command
​
spark = SparkSession.builder.appName("Clustering using SparkML").getOrCreate()
24/07/20 02:51:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/07/20 02:51:03 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Task 2 - Load the data in a csv file into a dataframe
Download the data file

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/customers.csv
​
--2024-07-20 02:51:09--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/customers.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 8909 (8.7K) [text/csv]
Saving to: ‘customers.csv’

customers.csv       100%[===================>]   8.70K  --.-KB/s    in 0s      

2024-07-20 02:51:09 (25.3 MB/s) - ‘customers.csv’ saved [8909/8909]

Load the dataset into the spark dataframe

# using the spark.read.csv function we load the data into a dataframe.
# the header = True mentions that there is a header row in out csv file
# the inferSchema = True, tells spark to automatically find out the data types of the columns.
​
# Load customers dataset
customer_data = spark.read.csv("customers.csv", header=True, inferSchema=True)
​
                                                                                
Print the schema of the dataset

# Each row in this dataset is about a customer. The columns indicate the orders placed
# by a customer for Fresh_food, Milk, Grocery and Frozen_Food
customer_data.printSchema()
root
 |-- Fresh_Food: integer (nullable = true)
 |-- Milk: integer (nullable = true)
 |-- Grocery: integer (nullable = true)
 |-- Frozen_Food: integer (nullable = true)

Show top 5 rows from the dataset

customer_data.show(n=5, truncate=False)
Task 3 - Create a feature vector
# Assemble the features into a single vector column
feature_cols = ['Fresh_Food', 'Milk', 'Grocery', 'Frozen_Food']
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
customer_transformed_data = assembler.transform(customer_data)
​
You must tell the KMeans algorithm how many clusters to create out of your data

number_of_clusters = 3
Task 4 - Create a clustering model
Create a KMeans clustering model

kmeans = KMeans(k = number_of_clusters)
​
Train/Fit the model on the dataset

model = kmeans.fit(customer_transformed_data)
​
24/07/20 02:52:20 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
24/07/20 02:52:20 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
                                                                                
Task 5 - Print Cluster Details
Your model is now trained. Time to evaluate the model.

# Make predictions on the dataset
predictions = model.transform(customer_transformed_data)
# Display the results
predictions.show(5)
+----------+----+-------+-----------+--------------------+----------+
|Fresh_Food|Milk|Grocery|Frozen_Food|            features|prediction|
+----------+----+-------+-----------+--------------------+----------+
|     12669|9656|   7561|        214|[12669.0,9656.0,7...|         0|
|      7057|9810|   9568|       1762|[7057.0,9810.0,95...|         0|
|      6353|8808|   7684|       2405|[6353.0,8808.0,76...|         0|
|     13265|1196|   4221|       6404|[13265.0,1196.0,4...|         0|
|     22615|5410|   7198|       3915|[22615.0,5410.0,7...|         2|
+----------+----+-------+-----------+--------------------+----------+
only showing top 5 rows

Display how many customers are there in each cluster.

predictions.groupBy('prediction').count().show()
[Stage 42:=========================================>              (56 + 8) / 75]
+----------+-----+
|prediction|count|
+----------+-----+
|         1|   49|
|         2|   60|
|         0|  331|
+----------+-----+

                                                                                
#stop spark session
spark.stop()
Exercises
Exercise 1 - Create a spark session
Create SparkSession with appname "Seed Clustering"

spark = #TODO
spark = SparkSession.builder.appName("Seed Clustering").getOrCreate()
24/07/20 02:54:33 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.

Exercise 2 - Load the data in a csv file into a dataframe
#download seed dataset
!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/seeds.csv
​
--2024-07-20 02:54:35--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/seeds.csv
Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 8973 (8.8K) [text/csv]
Saving to: ‘seeds.csv’

seeds.csv           100%[===================>]   8.76K  --.-KB/s    in 0.007s  

2024-07-20 02:54:36 (1.30 MB/s) - ‘seeds.csv’ saved [8973/8973]

Load the seed dataset



​
seed_data = spark.read.csv("seeds.csv", header=True, inferSchema=True)

Print the schema of the dataset

seed_data.printSchema()
root
 |-- area: double (nullable = true)
 |-- perimeter: double (nullable = true)
 |-- compactness: double (nullable = true)
 |-- length of kernel: double (nullable = true)
 |-- width of kernel: double (nullable = true)
 |-- asymmetry coefficient: double (nullable = true)
 |-- length of kernel groove: double (nullable = true)

Show top 5 rows of the data set

seed_data.show(n=5, truncate=False, vertical=True)
-RECORD 0-------------------------
 area                    | 15.26  
 perimeter               | 14.84  
 compactness             | 0.871  
 length of kernel        | 5.763  
 width of kernel         | 3.312  
 asymmetry coefficient   | 2.221  
 length of kernel groove | 5.22   
-RECORD 1-------------------------
 area                    | 14.88  
 perimeter               | 14.57  
 compactness             | 0.8811 
 length of kernel        | 5.554  
 width of kernel         | 3.333  
 asymmetry coefficient   | 1.018  
 length of kernel groove | 4.956  
-RECORD 2-------------------------
 area                    | 14.29  
 perimeter               | 14.09  
 compactness             | 0.905  
 length of kernel        | 5.291  
 width of kernel         | 3.337  
 asymmetry coefficient   | 2.699  
 length of kernel groove | 4.825  
-RECORD 3-------------------------
 area                    | 13.84  
 perimeter               | 13.94  
 compactness             | 0.8955 
 length of kernel        | 5.324  
 width of kernel         | 3.379  
 asymmetry coefficient   | 2.259  
 length of kernel groove | 4.805  
-RECORD 4-------------------------
 area                    | 16.14  
 perimeter               | 14.99  
 compactness             | 0.9034 
 length of kernel        | 5.658  
 width of kernel         | 3.562  
 asymmetry coefficient   | 1.355  
 length of kernel groove | 5.175  
only showing top 5 rows

Exercise 3 - Create a feature vector
Assemble all columns into a single vector



feature_cols = ['area',
 'perimeter',
 'compactness',
 'length of kernel',
 'width of kernel',
 'asymmetry coefficient',
 'length of kernel groove']
​
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
seed_transformed_data = assembler.transform(seed_data)

Exercise 4 - Create a clustering model
Create 7 clusters



number_of_clusters = 3
kmeans = KMeans(k = number_of_clusters)
model = kmeans.fit(seed_transformed_data)
                                                                                

Exercise 5 - Print Cluster Details
predictions =  #TODOpredictions = model.transform(seed_transformed_data)
predictions = model.transform(seed_transformed_data)

predictions.show(n=5, truncate=False, vertical=True)
-RECORD 0---------------------------------------------------------------
 area                    | 15.26                                        
 perimeter               | 14.84                                        
 compactness             | 0.871                                        
 length of kernel        | 5.763                                        
 width of kernel         | 3.312                                        
 asymmetry coefficient   | 2.221                                        
 length of kernel groove | 5.22                                         
 features                | [15.26,14.84,0.871,5.763,3.312,2.221,5.22]   
 prediction              | 0                                            
-RECORD 1---------------------------------------------------------------
 area                    | 14.88                                        
 perimeter               | 14.57                                        
 compactness             | 0.8811                                       
 length of kernel        | 5.554                                        
 width of kernel         | 3.333                                        
 asymmetry coefficient   | 1.018                                        
 length of kernel groove | 4.956                                        
 features                | [14.88,14.57,0.8811,5.554,3.333,1.018,4.956] 
 prediction              | 0                                            
-RECORD 2---------------------------------------------------------------
 area                    | 14.29                                        
 perimeter               | 14.09                                        
 compactness             | 0.905                                        
 length of kernel        | 5.291                                        
 width of kernel         | 3.337                                        
 asymmetry coefficient   | 2.699                                        
 length of kernel groove | 4.825                                        
 features                | [14.29,14.09,0.905,5.291,3.337,2.699,4.825]  
 prediction              | 0                                            
-RECORD 3---------------------------------------------------------------
 area                    | 13.84                                        
 perimeter               | 13.94                                        
 compactness             | 0.8955                                       
 length of kernel        | 5.324                                        
 width of kernel         | 3.379                                        
 asymmetry coefficient   | 2.259                                        
 length of kernel groove | 4.805                                        
 features                | [13.84,13.94,0.8955,5.324,3.379,2.259,4.805] 
 prediction              | 0                                            
-RECORD 4---------------------------------------------------------------
 area                    | 16.14                                        
 perimeter               | 14.99                                        
 compactness             | 0.9034                                       
 length of kernel        | 5.658                                        
 width of kernel         | 3.562                                        
 asymmetry coefficient   | 1.355                                        
 length of kernel groove | 5.175                                        
 features                | [16.14,14.99,0.9034,5.658,3.562,1.355,5.175] 
 prediction              | 0                                            
only showing top 5 rows

predictions.groupBy('prediction').count().show()
[Stage 29:==================================================>     (68 + 7) / 75]
+----------+-----+
|prediction|count|
+----------+-----+
|         1|   82|
|         2|   61|
|         0|   67|
+----------+-----+

                                                                                
#stop spark session
spark.stop()
#stop spark session
spark.stop()
